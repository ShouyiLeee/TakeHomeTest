{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca2ce76",
   "metadata": {},
   "source": [
    "# Sys check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb81344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# !pip install matplotlib \n",
    "# !pip install opencv-python\n",
    "# !pip install scikit-learn\n",
    "# !pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a28f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IT\\\\GITHUB\\\\TakeHomeTest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "root = os.getcwd()\n",
    "root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df12d722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 12 10:07:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   56C    P8              5W /   50W |     806MiB /   4096MiB |     23%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      4360    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      6756    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9560    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     12024    C+G   ...r\\AppData\\Roaming\\Zoom\\bin\\Zoom.exe      N/A      |\n",
      "|    0   N/A  N/A     13680    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     14904    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A     15112    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     17332    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17416    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17772    C+G   ... Access Service\\ePowerButton_NB.exe      N/A      |\n",
      "|    0   N/A  N/A     20688    C+G   ...71.0_x64__8wekyb3d8bbwe\\GameBar.exe      N/A      |\n",
      "|    0   N/A  N/A     21132    C+G   ...on\\135.0.3179.54\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     22468    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8b3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a76c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, criterion, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"Validation performance:\")\n",
    "        evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d48ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17226aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ced1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đánh giá\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2352e",
   "metadata": {},
   "source": [
    "# Single-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fde18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd7631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải dữ liệu\n",
    "data_dir = \"dataset\"\n",
    "train_set = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_set = datasets.ImageFolder(os.path.join(data_dir, \"dev\"), transform=transform)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20185163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Thay thế lớp fully-connected cuối cùng\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdfd9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639948e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Loss: 0.5858\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.78      0.84      0.81       602\n",
      "       spoof       0.83      0.77      0.79       602\n",
      "\n",
      "    accuracy                           0.80      1204\n",
      "   macro avg       0.80      0.80      0.80      1204\n",
      "weighted avg       0.80      0.80      0.80      1204\n",
      "\n",
      "[Epoch 2/10] Loss: 0.4750\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.77      0.88      0.82       602\n",
      "       spoof       0.86      0.74      0.80       602\n",
      "\n",
      "    accuracy                           0.81      1204\n",
      "   macro avg       0.81      0.81      0.81      1204\n",
      "weighted avg       0.81      0.81      0.81      1204\n",
      "\n",
      "[Epoch 3/10] Loss: 0.4388\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.77      0.89      0.83       602\n",
      "       spoof       0.87      0.74      0.80       602\n",
      "\n",
      "    accuracy                           0.81      1204\n",
      "   macro avg       0.82      0.81      0.81      1204\n",
      "weighted avg       0.82      0.81      0.81      1204\n",
      "\n",
      "[Epoch 4/10] Loss: 0.4125\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.85      0.82      0.84       602\n",
      "       spoof       0.83      0.85      0.84       602\n",
      "\n",
      "    accuracy                           0.84      1204\n",
      "   macro avg       0.84      0.84      0.84      1204\n",
      "weighted avg       0.84      0.84      0.84      1204\n",
      "\n",
      "[Epoch 5/10] Loss: 0.4125\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.87      0.77      0.82       602\n",
      "       spoof       0.79      0.89      0.84       602\n",
      "\n",
      "    accuracy                           0.83      1204\n",
      "   macro avg       0.83      0.83      0.83      1204\n",
      "weighted avg       0.83      0.83      0.83      1204\n",
      "\n",
      "[Epoch 6/10] Loss: 0.3937\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.87      0.83      0.85       602\n",
      "       spoof       0.84      0.88      0.86       602\n",
      "\n",
      "    accuracy                           0.86      1204\n",
      "   macro avg       0.86      0.86      0.86      1204\n",
      "weighted avg       0.86      0.86      0.86      1204\n",
      "\n",
      "[Epoch 7/10] Loss: 0.4180\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.88      0.80      0.84       602\n",
      "       spoof       0.82      0.89      0.85       602\n",
      "\n",
      "    accuracy                           0.85      1204\n",
      "   macro avg       0.85      0.85      0.85      1204\n",
      "weighted avg       0.85      0.85      0.85      1204\n",
      "\n",
      "[Epoch 8/10] Loss: 0.3884\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.94      0.80       602\n",
      "       spoof       0.91      0.59      0.72       602\n",
      "\n",
      "    accuracy                           0.76      1204\n",
      "   macro avg       0.80      0.76      0.76      1204\n",
      "weighted avg       0.80      0.76      0.76      1204\n",
      "\n",
      "[Epoch 9/10] Loss: 0.3994\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.92      0.65      0.76       602\n",
      "       spoof       0.73      0.94      0.82       602\n",
      "\n",
      "    accuracy                           0.79      1204\n",
      "   macro avg       0.82      0.79      0.79      1204\n",
      "weighted avg       0.82      0.79      0.79      1204\n",
      "\n",
      "[Epoch 10/10] Loss: 0.3648\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.88      0.74      0.81       602\n",
      "       spoof       0.78      0.90      0.83       602\n",
      "\n",
      "    accuracy                           0.82      1204\n",
      "   macro avg       0.83      0.82      0.82      1204\n",
      "weighted avg       0.83      0.82      0.82      1204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=num_epochs, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4b69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "torch.save(model.state_dict(), \"resnet50_liveness_single.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ceaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model's state dictionary\n",
    "# model.load_state_dict(torch.load(\"resnet50_liveness_single.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55763fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.88      0.78      0.83       602\n",
      "       spoof       0.80      0.90      0.85       602\n",
      "\n",
      "    accuracy                           0.84      1204\n",
      "   macro avg       0.84      0.84      0.84      1204\n",
      "weighted avg       0.84      0.84      0.84      1204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306220d",
   "metadata": {},
   "source": [
    "# Multi-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class MultiImageLivenessDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset for multi-image liveness detection.\n",
    "\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing subdirectories for each class ('normal' and 'spoof').\n",
    "            transform (callable, optional): Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.samples = []  # List to store image paths and labels\n",
    "        self.transform = transform  # Transformations to apply to images\n",
    "\n",
    "        # Iterate over the two classes: 'normal' and 'spoof'\n",
    "        for label_dir in [\"normal\", \"spoof\"]:\n",
    "            full_path = os.path.join(root_dir, label_dir)  # Full path to the class directory\n",
    "            persons = {}  # Dictionary to group images by person ID\n",
    "\n",
    "            # Group images by person ID\n",
    "            for img_path in glob.glob(os.path.join(full_path, \"*.jpg\")):\n",
    "                filename = os.path.basename(img_path)  # Extract the filename\n",
    "                person_id = filename.split(\"_\")[0]  # Extract the person ID from the filename\n",
    "                if person_id not in persons:\n",
    "                    persons[person_id] = []  # Initialize a list for the person ID\n",
    "                persons[person_id].append(img_path)  # Add the image path to the person's list\n",
    "\n",
    "            # Process each person's images\n",
    "            for person_id, images in persons.items():\n",
    "                images = sorted(images)  # Sort images for consistency\n",
    "                if len(images) >= 4:\n",
    "                    selected = images[:4]  # Select the first 4 images if there are enough\n",
    "                else:\n",
    "                    # If fewer than 4 images, duplicate the first image to make up the difference\n",
    "                    selected = (images + [images[0]] * 4)[:4]\n",
    "                # Append the selected images and label (0 for 'normal', 1 for 'spoof') to the samples list\n",
    "                self.samples.append((selected, 0 if label_dir == \"normal\" else 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing a tensor of stacked images and the corresponding label.\n",
    "        \"\"\"\n",
    "        img_paths, label = self.samples[idx]  # Get the image paths and label for the given index\n",
    "        imgs = []  # List to store the processed images\n",
    "\n",
    "        # Load and process each image\n",
    "        for path in img_paths:\n",
    "            image = Image.open(path).convert(\"RGB\")  # Open the image and convert it to RGB\n",
    "            if self.transform:\n",
    "                image = self.transform(image)  # Apply transformations if specified\n",
    "            imgs.append(image)  # Add the processed image to the list\n",
    "\n",
    "        # Stack the images into a single tensor and return it along with the label\n",
    "        return torch.stack(imgs), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e577c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MultiImageLivenessDataset(\"dataset/train\", transform=transform)\n",
    "val_set = MultiImageLivenessDataset(\"dataset/dev\", transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9805308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đánh giá\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30828e4",
   "metadata": {},
   "source": [
    "## Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c957d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MultiImageResNetLSTM(nn.Module):\n",
    "    def __init__(self, base_model, lstm_hidden=512, num_classes=2):\n",
    "        \"\"\"\n",
    "        A model combining ResNet for feature extraction and LSTM for temporal modeling.\n",
    "\n",
    "        Args:\n",
    "            base_model: Pretrained ResNet model.\n",
    "            lstm_hidden: Number of hidden units in the LSTM layer.\n",
    "            num_classes: Number of output classes for classification.\n",
    "        \"\"\"\n",
    "        super(MultiImageResNetLSTM, self).__init__()\n",
    "        # Use ResNet as a feature extractor, removing the fully connected (FC) layer\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # ResNet50 without FC\n",
    "        \n",
    "        # LSTM layer for temporal modeling of features\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=lstm_hidden, batch_first=True)\n",
    "        \n",
    "        # Fully connected classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 256),  # Linear layer to reduce dimensions\n",
    "            nn.ReLU(),                   # Activation function\n",
    "            nn.Dropout(0.4),             # Dropout for regularization\n",
    "            nn.Linear(256, num_classes)  # Final layer for classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [B, 4, C, H, W], where:\n",
    "               B = Batch size, 4 = Number of images per sample, \n",
    "               C = Channels, H = Height, W = Width.\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [B, num_classes].\n",
    "        \"\"\"\n",
    "        # Reshape input to process all images in the batch\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)  # [B * 4, C, H, W]\n",
    "\n",
    "        # Extract features using ResNet\n",
    "        features = self.feature_extractor(x)  # [B * 4, 2048, 1, 1]\n",
    "        features = features.view(B, N, -1)    # Reshape to [B, 4, 2048]\n",
    "\n",
    "        # Pass features through LSTM\n",
    "        _, (hn, _) = self.lstm(features)      # hn: [1, B, lstm_hidden]\n",
    "        hn = hn.squeeze(0)                    # Remove the first dimension: [B, lstm_hidden]\n",
    "\n",
    "        # Classify using the fully connected layers\n",
    "        out = self.classifier(hn)             # [B, num_classes]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c448f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "model = MultiImageResNetLSTM(resnet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23e33925",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a78b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/5] Loss: 0.5239\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.91      0.90      0.90       201\n",
      "       spoof       0.90      0.91      0.91       204\n",
      "\n",
      "    accuracy                           0.90       405\n",
      "   macro avg       0.90      0.90      0.90       405\n",
      "weighted avg       0.90      0.90      0.90       405\n",
      "\n",
      "[Epoch 2/5] Loss: 0.3656\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.95      0.88      0.91       201\n",
      "       spoof       0.89      0.96      0.92       204\n",
      "\n",
      "    accuracy                           0.92       405\n",
      "   macro avg       0.92      0.92      0.92       405\n",
      "weighted avg       0.92      0.92      0.92       405\n",
      "\n",
      "[Epoch 3/5] Loss: 0.2872\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.83      0.96      0.89       201\n",
      "       spoof       0.95      0.81      0.88       204\n",
      "\n",
      "    accuracy                           0.88       405\n",
      "   macro avg       0.89      0.88      0.88       405\n",
      "weighted avg       0.89      0.88      0.88       405\n",
      "\n",
      "[Epoch 4/5] Loss: 0.2112\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.93      0.58      0.72       201\n",
      "       spoof       0.70      0.96      0.81       204\n",
      "\n",
      "    accuracy                           0.77       405\n",
      "   macro avg       0.81      0.77      0.76       405\n",
      "weighted avg       0.81      0.77      0.76       405\n",
      "\n",
      "[Epoch 5/5] Loss: 0.2362\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.93      0.94       201\n",
      "       spoof       0.93      0.94      0.94       204\n",
      "\n",
      "    accuracy                           0.94       405\n",
      "   macro avg       0.94      0.94      0.94       405\n",
      "weighted avg       0.94      0.94      0.94       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=5, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4b9e3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.95      0.94      0.94       201\n",
      "       spoof       0.94      0.95      0.94       204\n",
      "\n",
      "    accuracy                           0.94       405\n",
      "   macro avg       0.94      0.94      0.94       405\n",
      "weighted avg       0.94      0.94      0.94       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b306027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_roc_auc(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true, y_scores = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_group, labels in dataloader:\n",
    "            image_group = image_group.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(image_group)\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_scores.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "    return roc_auc\n",
    "\n",
    "# Calculate ROC-AUC for the validation set\n",
    "roc_auc = calculate_roc_auc(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b507c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"resnet50_liveness_multi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37acb90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the model is initialized correctly\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "model = MultiImageResNetLSTM(resnet).to(device)\n",
    "\n",
    "# Load the state dictionary\n",
    "model.load_state_dict(torch.load(\"resnet50_liveness_multi.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407bedb",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4e783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image spoof probability: 0.0427\n",
      "Multiple images spoof probability: 0.9030\n",
      "Single image classification: Normal (real)\n",
      "Multiple images classification: Spoof (fake)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def predict_spoof_probability(input_path, model, transform, device):\n",
    "    \"\"\"\n",
    "    Predict if an image or group of images is normal or spoof using the multi-image model.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Can be either:\n",
    "                   - Single image path (string)\n",
    "                   - List of image paths for the same person\n",
    "                   - Directory containing images of the same person\n",
    "        model: Trained MultiImageResNetLSTM model\n",
    "        transform: Preprocessing transforms\n",
    "        device: Device to run inference on ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        A probability in range [0,1] where:\n",
    "        - 0 indicates normal (real face)\n",
    "        - 1 indicates spoof (fake face)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Determine input type\n",
    "    if isinstance(input_path, str):\n",
    "        if os.path.isfile(input_path):\n",
    "            # Single image path\n",
    "            img_paths = [input_path]\n",
    "        elif os.path.isdir(input_path):\n",
    "            # Directory of images\n",
    "            img_paths = sorted(glob.glob(os.path.join(input_path, \"*.jpg\")))\n",
    "            if not img_paths:\n",
    "                raise ValueError(f\"No images found in directory: {input_path}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Input path does not exist: {input_path}\")\n",
    "    elif isinstance(input_path, list):\n",
    "        # List of image paths\n",
    "        img_paths = input_path\n",
    "    else:\n",
    "        raise ValueError(\"Input path must be a string or list of strings\")\n",
    "    \n",
    "    # Process images\n",
    "    processed_images = []\n",
    "    for path in img_paths:\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if transform:\n",
    "            image = transform(image)\n",
    "        processed_images.append(image)\n",
    "    \n",
    "    # Handle case where we have fewer than 4 images\n",
    "    if len(processed_images) < 4:\n",
    "        # Duplicate first image to get 4 total images\n",
    "        while len(processed_images) < 4:\n",
    "            processed_images.append(processed_images[0])\n",
    "    elif len(processed_images) > 4:\n",
    "        # If more than 4 images, use only the first 4\n",
    "        processed_images = processed_images[:4]\n",
    "    \n",
    "    # Stack images and prepare for model\n",
    "    image_batch = torch.stack(processed_images).unsqueeze(0).to(device)  # [1, 4, C, H, W]\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_batch)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "    # Return spoof probability (index 1)\n",
    "    return probabilities[0, 1].item()\n",
    "\n",
    "\n",
    "\n",
    "# Example paths (replace with actual paths)\n",
    "single_image = \"D:\\\\IT\\\\GITHUB\\\\TakeHomeTest\\\\dataset\\\\dev\\\\normal\\\\2_4.jpg\"\n",
    "score1 = predict_spoof_probability(single_image, model, transform, device)\n",
    "print(f\"Single image spoof probability: {score1:.4f}\")\n",
    "print(f\"Single image classification: {'Spoof (fake)' if score1 > 0.5 else 'Normal (real)'}\")\n",
    "\n",
    "\n",
    "multiple_images = [\n",
    "    \"D:\\\\IT\\\\GITHUB\\\\TakeHomeTest\\\\dataset\\\\dev\\\\spoof\\\\63_1.jpg\",\n",
    "    \"D:\\\\IT\\\\GITHUB\\\\TakeHomeTest\\\\dataset\\\\dev\\\\spoof\\\\63_2.jpg\",\n",
    "    \"D:\\\\IT\\\\GITHUB\\\\TakeHomeTest\\\\dataset\\\\dev\\\\spoof\\\\63_3.jpg\",\n",
    "    \"D:\\\\IT\\\\GITHUB\\\\TakeHomeTest\\\\dataset\\\\dev\\\\spoof\\\\63_4.jpg\"\n",
    "]\n",
    "score2 = predict_spoof_probability(multiple_images, model, transform, device)\n",
    "print(f\"Multiple images spoof probability: {score2:.4f}\")\n",
    "print(f\"Multiple images classification: {'Spoof (fake)' if score2 > 0.5 else 'Normal (real)'}\")\n",
    "\n",
    "\n",
    "# person_dir = \"D:\\\\IT\\\\GITHUB\\\\TakeHomeTest\\\\dataset\\\\dev\\\\spoof\\\\person_a\"\n",
    "# # Example directory (replace with actual directory)\n",
    "# score3 = predict_spoof_probability(person_dir, model, transform, device)   \n",
    "# print(f\"Directory spoof probability: {score2:.4f}\")\n",
    "# print(f\"Directory classification: {'Spoof (fake)' if score2 > 0.5 else 'Normal (real)'}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee885e9",
   "metadata": {},
   "source": [
    "## VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f54b611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViT_LivenessClassifier(nn.Module):\n",
    "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=2):\n",
    "        super(ViT_LivenessClassifier, self).__init__()\n",
    "        self.vit = timm.create_model(vit_model_name, pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # Bỏ classification head của ViT\n",
    "\n",
    "        self.embedding_dim = self.vit.num_features  # Thường là 768\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, C, H, W]\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "\n",
    "        embeddings = self.vit(x)  # [B*4, D]\n",
    "        embeddings = embeddings.view(B, N, -1)  # [B, 4, D]\n",
    "\n",
    "        # Mean pooling over 4 embeddings\n",
    "        pooled = embeddings.mean(dim=1)  # [B, D]\n",
    "        out = self.classifier(pooled)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f0392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = ViT_LivenessClassifier(vit_model_name='vit_base_patch16_224').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6d17caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/2] Loss: 0.1505\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.96      0.95       201\n",
      "       spoof       0.96      0.94      0.95       204\n",
      "\n",
      "    accuracy                           0.95       405\n",
      "   macro avg       0.95      0.95      0.95       405\n",
      "weighted avg       0.95      0.95      0.95       405\n",
      "\n",
      "[Epoch 2/2] Loss: 0.1607\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.96      0.95       201\n",
      "       spoof       0.96      0.94      0.95       204\n",
      "\n",
      "    accuracy                           0.95       405\n",
      "   macro avg       0.95      0.95      0.95       405\n",
      "weighted avg       0.95      0.95      0.95       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5acd4e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.96      0.95       201\n",
      "       spoof       0.96      0.94      0.95       204\n",
      "\n",
      "    accuracy                           0.95       405\n",
      "   macro avg       0.95      0.95      0.95       405\n",
      "weighted avg       0.95      0.95      0.95       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d37a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"vit_liveness_multi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"vit_liveness_multi.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b84877",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAutoEncoder, self).__init__()\n",
    "        # Encoder: Reduces the spatial dimensions while increasing the feature depth\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),  # Convolution: [B, 3, 224, 224] -> [B, 32, 112, 112]\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # Convolution: [B, 32, 112, 112] -> [B, 64, 56, 56]\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # Convolution: [B, 64, 56, 56] -> [B, 128, 28, 28]\n",
    "            nn.ReLU()  # Activation function\n",
    "        )\n",
    "        # Decoder: Reconstructs the original image from the encoded representation\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # Transposed Convolution: [B, 128, 28, 28] -> [B, 64, 56, 56]\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # Transposed Convolution: [B, 64, 56, 56] -> [B, 32, 112, 112]\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),  # Transposed Convolution: [B, 32, 112, 112] -> [B, 3, 224, 224]\n",
    "            nn.Sigmoid()  # Activation function to normalize pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        x = self.encoder(x)\n",
    "        # Forward pass through the decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504907e",
   "metadata": {},
   "source": [
    "### Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Use only normal images for training\n",
    "class NormalOnlyDataset(MultiImageLivenessDataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__(root_dir, transform)\n",
    "        self.samples = [s for s in self.samples if s[1] == 0]\n",
    "\n",
    "train_dataset = NormalOnlyDataset(\"dataset/train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Use all images for validation\n",
    "val_dataset = MultiImageLivenessDataset(\"dataset/dev\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "700ffe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleAutoEncoder().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e109515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0610\n",
      "Epoch 2, Loss: 0.0240\n",
      "Epoch 3, Loss: 0.0092\n",
      "Epoch 4, Loss: 0.0069\n",
      "Epoch 5, Loss: 0.0056\n",
      "Epoch 6, Loss: 0.0048\n",
      "Epoch 7, Loss: 0.0041\n",
      "Epoch 8, Loss: 0.0037\n",
      "Epoch 9, Loss: 0.0036\n",
      "Epoch 10, Loss: 0.0031\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for image_group, _ in train_loader:  # image_group: [B, 4, 3, H, W]\n",
    "        images = image_group.view(-1, 3, 224, 224).to(device)  # [B*4, 3, H, W]\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c9d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_autoencoder(model, dataloader, threshold=None):\n",
    "    \"\"\"\n",
    "    Evaluate the autoencoder model for liveness detection.\n",
    "\n",
    "    Args:\n",
    "        model: The trained autoencoder model.\n",
    "        dataloader: DataLoader for the dataset to evaluate.\n",
    "        threshold: Threshold for classification. If None, the function will find the best threshold.\n",
    "\n",
    "    Returns:\n",
    "        If threshold is None, returns the best threshold. Otherwise, prints the classification report.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_true, y_pred, scores = [], [], []  # Initialize lists to store true labels, predicted labels, and scores\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for image_group, labels in dataloader:\n",
    "            batch_size = image_group.size(0)  # Get the batch size\n",
    "            # Flatten the image group for processing: [B*4, 3, 224, 224]\n",
    "            images = image_group.view(-1, 3, 224, 224).to(device)\n",
    "            # Reconstruct the images using the autoencoder\n",
    "            recons = model(images)\n",
    "\n",
    "            # Compute the Mean Squared Error (MSE) for each image: [B*4]\n",
    "            mse = F.mse_loss(recons, images, reduction='none')\n",
    "            mse = mse.view(batch_size, 4, -1).mean(dim=2)  # Reshape and compute mean MSE for each image group: [B, 4]\n",
    "            group_mse = mse.mean(dim=1)  # Compute the mean MSE for the group: [B]\n",
    "\n",
    "            # Classification based on the threshold\n",
    "            if threshold is None:\n",
    "                # If no threshold is provided, store scores and true labels for threshold optimization\n",
    "                scores.extend(group_mse.cpu().numpy())\n",
    "                y_true.extend(labels.numpy())\n",
    "            else:\n",
    "                # Classify based on the threshold\n",
    "                pred_labels = (group_mse > threshold).long()  # Predict labels: 1 if MSE > threshold, else 0\n",
    "                y_pred.extend(pred_labels.cpu().numpy())\n",
    "                y_true.extend(labels.numpy())\n",
    "\n",
    "    if threshold is None:\n",
    "        # If no threshold is provided, find the best threshold using ROC curve\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, thres = roc_curve(y_true, scores)  # Compute False Positive Rate, True Positive Rate, and thresholds\n",
    "        best_idx = (tpr - fpr).argmax()  # Find the index of the best threshold (maximizing TPR - FPR)\n",
    "        best_threshold = thres[best_idx]  # Get the best threshold\n",
    "        print(f\"[INFO] Best threshold: {best_threshold:.4f}\")\n",
    "        return best_threshold  # Return the best threshold\n",
    "    else:\n",
    "        # If a threshold is provided, print the classification report\n",
    "        report = classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"])\n",
    "        print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best threshold: 0.0024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.53      0.49      0.51       201\n",
      "       spoof       0.53      0.57      0.55       204\n",
      "\n",
      "    accuracy                           0.53       405\n",
      "   macro avg       0.53      0.53      0.53       405\n",
      "weighted avg       0.53      0.53      0.53       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find best threshold\n",
    "best_thresh = evaluate_autoencoder(model, val_loader, threshold=None)\n",
    "\n",
    "# evaluate with the best threshold\n",
    "evaluate_autoencoder(model, val_loader, threshold=best_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6a0a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"AEReconstruction_liveness_multi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cb5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"AEReconstruction_liveness_multi.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cb861",
   "metadata": {},
   "source": [
    "### AE+Resnet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55fb6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MultiImageLivenessDataset(\"dataset/train\", transform=transform)\n",
    "val_set = MultiImageLivenessDataset(\"dataset/dev\", transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        # Load a pre-trained ResNet18 model\n",
    "        base = resnet18(pretrained=True)\n",
    "        # Remove the final fully connected (FC) layer to use it as a feature extractor\n",
    "        base.fc = nn.Identity()\n",
    "        self.backbone = base\n",
    "        # Add a new classifier layer for binary classification (2 classes)\n",
    "        self.classifier = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features using the ResNet backbone\n",
    "        feat = self.backbone(x)\n",
    "        # Pass the extracted features through the classifier\n",
    "        out = self.classifier(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27d61848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_ResNet_Pipeline(nn.Module):\n",
    "    def __init__(self, autoencoder, classifier):\n",
    "        super(AE_ResNet_Pipeline, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, 3, 224, 224]\n",
    "        b, s, c, h, w = x.size()\n",
    "        x = x.view(-1, c, h, w)  # [B*4, 3, H, W]\n",
    "\n",
    "        # AE reconstruct\n",
    "        recon = self.autoencoder(x)  # [B*4, 3, H, W]\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.classifier(recon)  # [B*4, 2]\n",
    "\n",
    "        logits = logits.view(b, s, 2).mean(dim=1)  # [B, 2], avg over 4 imgs\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d867070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = AE_ResNet_Pipeline(SimpleAutoEncoder(), ResNetClassifier()).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "177671ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.6647\n",
      "[Epoch 2] Loss: 0.5729\n",
      "[Epoch 3] Loss: 0.5718\n",
      "[Epoch 4] Loss: 0.4925\n",
      "[Epoch 5] Loss: 0.4539\n",
      "[Epoch 6] Loss: 0.3784\n",
      "[Epoch 7] Loss: 0.3457\n",
      "[Epoch 8] Loss: 0.2916\n",
      "[Epoch 9] Loss: 0.2515\n",
      "[Epoch 10] Loss: 0.1931\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for image_group, labels in train_loader:  # [B, 4, 3, H, W]\n",
    "        image_group = image_group.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(image_group)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc329a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_pipeline(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_group, labels in dataloader:\n",
    "            image_group = image_group.to(device)\n",
    "            outputs = model(image_group)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7b8b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.89      0.49      0.63       201\n",
      "       spoof       0.65      0.94      0.77       204\n",
      "\n",
      "    accuracy                           0.72       405\n",
      "   macro avg       0.77      0.71      0.70       405\n",
      "weighted avg       0.77      0.72      0.70       405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "212869b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"AEClassifier_liveness_multi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"AeClassifier_liveness_multi.pth\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Takehometest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
