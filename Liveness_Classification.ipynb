{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca2ce76",
   "metadata": {},
   "source": [
    "# Sys check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a28f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IT\\\\GITHUB\\\\TakeHomeTest'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "root = os.getcwd()\n",
    "root "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df12d722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 11 20:21:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8              5W /   50W |     613MiB /   4096MiB |     23%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      5680    C+G   C:\\Windows\\System32\\ShellHost.exe           N/A      |\n",
      "|    0   N/A  N/A      8720    C+G   ... Access Service\\ePowerButton_NB.exe      N/A      |\n",
      "|    0   N/A  N/A      9448    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     10928    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A     11952    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     12676    C+G   ...\\Programs\\Zalo\\Zalo-25.3.2\\Zalo.exe      N/A      |\n",
      "|    0   N/A  N/A     14444    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     17692    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     20496    C+G   ...25.3.2\\plugins\\capture\\ZaloCall.exe      N/A      |\n",
      "|    0   N/A  N/A     20880    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "|    0   N/A  N/A     21692    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     22096    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     24932    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8b3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a76c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, optimizer, criterion, train_loader, val_loader, num_epochs=10, learning_rate=0.001, device='cuda'):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        print(\"Validation performance:\")\n",
    "        evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2352e",
   "metadata": {},
   "source": [
    "# Single-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fde18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17226aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d48ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean/std\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd7631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tải dữ liệu\n",
    "data_dir = \"dataset\"\n",
    "train_set = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_set = datasets.ImageFolder(os.path.join(data_dir, \"dev\"), transform=transform)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20185163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Thay thế lớp fully-connected cuối cùng\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_classes)\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfd9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ced1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đánh giá\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639948e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Loss: 0.4960\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.83      0.80      0.81       602\n",
      "       spoof       0.81      0.83      0.82       602\n",
      "\n",
      "    accuracy                           0.82      1204\n",
      "   macro avg       0.82      0.82      0.82      1204\n",
      "weighted avg       0.82      0.82      0.82      1204\n",
      "\n",
      "[Epoch 2/10] Loss: 0.4300\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.90      0.66      0.76       602\n",
      "       spoof       0.73      0.93      0.82       602\n",
      "\n",
      "    accuracy                           0.79      1204\n",
      "   macro avg       0.82      0.79      0.79      1204\n",
      "weighted avg       0.82      0.79      0.79      1204\n",
      "\n",
      "[Epoch 3/10] Loss: 0.3972\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.78      0.89      0.83       602\n",
      "       spoof       0.87      0.75      0.81       602\n",
      "\n",
      "    accuracy                           0.82      1204\n",
      "   macro avg       0.83      0.82      0.82      1204\n",
      "weighted avg       0.83      0.82      0.82      1204\n",
      "\n",
      "[Epoch 4/10] Loss: 0.3861\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.88      0.76      0.82       602\n",
      "       spoof       0.79      0.90      0.84       602\n",
      "\n",
      "    accuracy                           0.83      1204\n",
      "   macro avg       0.83      0.83      0.83      1204\n",
      "weighted avg       0.83      0.83      0.83      1204\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      7\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Huấn luyện\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"Validation performance:\")\n",
    "    evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "# torch.save(model.state_dict(), \"resnet50_liveness.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a55763fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 12\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     10\u001b[0m         _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m         y_true\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 12\u001b[0m         y_pred\u001b[38;5;241m.\u001b[39mextend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     14\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(y_true, y_pred, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspoof\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(report)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c306220d",
   "metadata": {},
   "source": [
    "# Multi-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class MultiImageLivenessDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        for label_dir in [\"normal\", \"spoof\"]:\n",
    "            full_path = os.path.join(root_dir, label_dir)\n",
    "            persons = {}\n",
    "\n",
    "            # Gom ảnh theo từng person_id\n",
    "            for img_path in glob.glob(os.path.join(full_path, \"*.jpg\")):\n",
    "                filename = os.path.basename(img_path)\n",
    "                person_id = filename.split(\"_\")[0]\n",
    "                if person_id not in persons:\n",
    "                    persons[person_id] = []\n",
    "                persons[person_id].append(img_path)\n",
    "\n",
    "            for person_id, images in persons.items():\n",
    "                images = sorted(images)\n",
    "                if len(images) >= 4:\n",
    "                    selected = images[:4]\n",
    "                else:\n",
    "                    # Nếu ít hơn 4 ảnh → nhân bản ảnh đầu tiên\n",
    "                    selected = (images + [images[0]] * 4)[:4]\n",
    "                self.samples.append((selected, 0 if label_dir == \"normal\" else 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_paths, label = self.samples[idx]\n",
    "        imgs = []\n",
    "\n",
    "        for path in img_paths:\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            imgs.append(image)\n",
    "\n",
    "        return torch.stack(imgs), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e577c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MultiImageLivenessDataset(\"dataset/train\", transform=transform)\n",
    "val_set = MultiImageLivenessDataset(\"dataset/dev\", transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9805308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm đánh giá\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"])\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30828e4",
   "metadata": {},
   "source": [
    "## Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c957d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class MultiImageResNetLSTM(nn.Module):\n",
    "    def __init__(self, base_model, lstm_hidden=512, num_classes=2):\n",
    "        super(MultiImageResNetLSTM, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # ResNet50 bỏ FC\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=lstm_hidden, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, C, H, W]\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "\n",
    "        features = self.feature_extractor(x)  # [B * 4, 2048, 1, 1]\n",
    "        features = features.view(B, N, -1)    # [B, 4, 2048]\n",
    "\n",
    "        _, (hn, _) = self.lstm(features)      # hn: [1, B, hidden]\n",
    "        hn = hn.squeeze(0)                    # [B, hidden]\n",
    "\n",
    "        out = self.classifier(hn)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c448f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "model = MultiImageResNetLSTM(resnet).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e33925",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe5a78b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3] Loss: 0.4829\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.83      0.92      0.88       130\n",
      "       spoof       0.91      0.82      0.86       130\n",
      "\n",
      "    accuracy                           0.87       260\n",
      "   macro avg       0.87      0.87      0.87       260\n",
      "weighted avg       0.87      0.87      0.87       260\n",
      "\n",
      "[Epoch 2/3] Loss: 0.3081\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.76      0.95      0.85       130\n",
      "       spoof       0.93      0.71      0.80       130\n",
      "\n",
      "    accuracy                           0.83       260\n",
      "   macro avg       0.85      0.83      0.82       260\n",
      "weighted avg       0.85      0.83      0.82       260\n",
      "\n",
      "[Epoch 3/3] Loss: 0.2774\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.87      0.95      0.91       130\n",
      "       spoof       0.94      0.86      0.90       130\n",
      "\n",
      "    accuracy                           0.90       260\n",
      "   macro avg       0.91      0.90      0.90       260\n",
      "weighted avg       0.91      0.90      0.90       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=3, learning_rate=0.001, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee885e9",
   "metadata": {},
   "source": [
    "## VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f54b611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ViT_LivenessClassifier(nn.Module):\n",
    "    def __init__(self, vit_model_name='vit_base_patch16_224', num_classes=2):\n",
    "        super(ViT_LivenessClassifier, self).__init__()\n",
    "        self.vit = timm.create_model(vit_model_name, pretrained=True)\n",
    "        self.vit.head = nn.Identity()  # Bỏ classification head của ViT\n",
    "\n",
    "        self.embedding_dim = self.vit.num_features  # Thường là 768\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, C, H, W]\n",
    "        B, N, C, H, W = x.shape\n",
    "        x = x.view(B * N, C, H, W)\n",
    "\n",
    "        embeddings = self.vit(x)  # [B*4, D]\n",
    "        embeddings = embeddings.view(B, N, -1)  # [B, 4, D]\n",
    "\n",
    "        # Mean pooling over 4 embeddings\n",
    "        pooled = embeddings.mean(dim=1)  # [B, D]\n",
    "        out = self.classifier(pooled)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f0392d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = ViT_LivenessClassifier(vit_model_name='vit_base_patch16_224').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6d17caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit_model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60985277",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),  # Hoặc theo ViT\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc48868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/3] Loss: 0.2134\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.93      0.91      0.92       130\n",
      "       spoof       0.91      0.93      0.92       130\n",
      "\n",
      "    accuracy                           0.92       260\n",
      "   macro avg       0.92      0.92      0.92       260\n",
      "weighted avg       0.92      0.92      0.92       260\n",
      "\n",
      "[Epoch 2/3] Loss: 0.2278\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.88      0.94      0.91       130\n",
      "       spoof       0.93      0.88      0.90       130\n",
      "\n",
      "    accuracy                           0.91       260\n",
      "   macro avg       0.91      0.91      0.91       260\n",
      "weighted avg       0.91      0.91      0.91       260\n",
      "\n",
      "[Epoch 3/3] Loss: 0.2521\n",
      "Validation performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.86      0.93      0.90       130\n",
      "       spoof       0.93      0.85      0.89       130\n",
      "\n",
      "    accuracy                           0.89       260\n",
      "   macro avg       0.89      0.89      0.89       260\n",
      "weighted avg       0.89      0.89      0.89       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training(model, optimizer, criterion, train_loader, val_loader, num_epochs=3, learning_rate=0.001, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b84877",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd71bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleAutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1),  # [B, 32, 112, 112]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # [B, 64, 56, 56]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),  # [B, 128, 28, 28]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # [B, 64, 56, 56]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # [B, 32, 112, 112]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),  # [B, 3, 224, 224]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504907e",
   "metadata": {},
   "source": [
    "### Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b629ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Chỉ dùng ảnh \"normal\" để huấn luyện AE\n",
    "class NormalOnlyDataset(MultiImageLivenessDataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        super().__init__(root_dir, transform)\n",
    "        self.samples = [s for s in self.samples if s[1] == 0]\n",
    "\n",
    "train_dataset = NormalOnlyDataset(\"dataset/train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Full dataset để đánh giá\n",
    "val_dataset = MultiImageLivenessDataset(\"dataset/dev\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "700ffe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleAutoEncoder().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e109515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0329\n",
      "Epoch 2, Loss: 0.0129\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for image_group, _ in train_loader:  # image_group: [B, 4, 3, H, W]\n",
    "        images = image_group.view(-1, 3, 224, 224).to(device)  # [B*4, 3, H, W]\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "31c9d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_autoencoder(model, dataloader, threshold=None):\n",
    "    model.eval()\n",
    "    y_true, y_pred, scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_group, labels in dataloader:\n",
    "            batch_size = image_group.size(0)\n",
    "            images = image_group.view(-1, 3, 224, 224).to(device)  # [B*4, 3, H, W]\n",
    "            recons = model(images)\n",
    "\n",
    "            # Tính lỗi từng ảnh: [B*4]\n",
    "            mse = F.mse_loss(recons, images, reduction='none')\n",
    "            mse = mse.view(batch_size, 4, -1).mean(dim=2)  # [B, 4]\n",
    "            group_mse = mse.mean(dim=1)  # [B]\n",
    "\n",
    "            # Phân loại\n",
    "            if threshold is None:\n",
    "                scores.extend(group_mse.cpu().numpy())\n",
    "                y_true.extend(labels.numpy())\n",
    "            else:\n",
    "                pred_labels = (group_mse > threshold).long()\n",
    "                y_pred.extend(pred_labels.cpu().numpy())\n",
    "                y_true.extend(labels.numpy())\n",
    "\n",
    "    if threshold is None:\n",
    "        # Auto tìm threshold tốt nhất\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, thres = roc_curve(y_true, scores)\n",
    "        best_idx = (tpr - fpr).argmax()\n",
    "        best_threshold = thres[best_idx]\n",
    "        print(f\"[INFO] Best threshold: {best_threshold:.4f}\")\n",
    "        return best_threshold\n",
    "    else:\n",
    "        report = classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"])\n",
    "        print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ff24ae08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Best threshold: 0.0109\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.58      0.82      0.68       130\n",
      "       spoof       0.69      0.41      0.51       130\n",
      "\n",
      "    accuracy                           0.61       260\n",
      "   macro avg       0.63      0.61      0.59       260\n",
      "weighted avg       0.63      0.61      0.59       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tìm ngưỡng phân biệt tốt nhất\n",
    "best_thresh = evaluate_autoencoder(model, val_loader, threshold=None)\n",
    "\n",
    "# Đánh giá với ngưỡng này\n",
    "evaluate_autoencoder(model, val_loader, threshold=best_thresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cb861",
   "metadata": {},
   "source": [
    "### AE+Resnet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "55fb6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MultiImageLivenessDataset(\"dataset/train\", transform=transform)\n",
    "val_set = MultiImageLivenessDataset(\"dataset/dev\", transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9eab17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetClassifier, self).__init__()\n",
    "        base = resnet18(pretrained=True)\n",
    "        base.fc = nn.Identity()  # remove final FC\n",
    "        self.backbone = base\n",
    "        self.classifier = nn.Linear(512, 2)  # binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        out = self.classifier(feat)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "27d61848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_ResNet_Pipeline(nn.Module):\n",
    "    def __init__(self, autoencoder, classifier):\n",
    "        super(AE_ResNet_Pipeline, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 4, 3, 224, 224]\n",
    "        b, s, c, h, w = x.size()\n",
    "        x = x.view(-1, c, h, w)  # [B*4, 3, H, W]\n",
    "\n",
    "        # AE reconstruct\n",
    "        recon = self.autoencoder(x)  # [B*4, 3, H, W]\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.classifier(recon)  # [B*4, 2]\n",
    "\n",
    "        logits = logits.view(b, s, 2).mean(dim=1)  # [B, 2], avg over 4 imgs\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d867070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\Takehometest\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = AE_ResNet_Pipeline(SimpleAutoEncoder(), ResNetClassifier()).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "177671ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.7765\n",
      "[Epoch 2] Loss: 0.5868\n",
      "[Epoch 3] Loss: 0.5260\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for image_group, labels in train_loader:  # [B, 4, 3, H, W]\n",
    "        image_group = image_group.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(image_group)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bc329a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate_pipeline(model, dataloader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_group, labels in dataloader:\n",
    "            image_group = image_group.to(device)\n",
    "            outputs = model(image_group)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"normal\", \"spoof\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7b8b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.75      0.05      0.09       130\n",
      "       spoof       0.51      0.98      0.67       130\n",
      "\n",
      "    accuracy                           0.52       260\n",
      "   macro avg       0.63      0.52      0.38       260\n",
      "weighted avg       0.63      0.52      0.38       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Takehometest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
